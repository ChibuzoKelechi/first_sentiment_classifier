{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "# nltk.download('punkt')\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'These': True, 'are': True, 'the': True, 'angels': True, '.': True}\n"
     ]
    }
   ],
   "source": [
    "def token_sentence(sent):\n",
    "    return ({word: True for word in nltk.word_tokenize(sent)})\n",
    "\n",
    "tokentxt = token_sentence('These are the angels.')\n",
    "\n",
    "print(tokentxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = []\n",
    "neg_tweets = []\n",
    "\n",
    "with open(\"pos_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        pos_tweets.append([token_sentence(i), 'positive'])\n",
    "\n",
    "with open('neg_tweets.txt') as nf:\n",
    "    for n in nf:\n",
    "        neg_tweets.append([token_sentence(n), 'negative'])\n",
    "\n",
    "len(pos_tweets)\n",
    "\n",
    "data = pd.read_csv('HateSpeechDatasetBalanced.csv')\n",
    "\n",
    "positive_data = data[data['Label'] == 0]\n",
    "negative_data = data[data['Label'] == 1]\n",
    "\n",
    "positive_data = positive_data['Content']\n",
    "negative_data = negative_data['Content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pos_tweets[:int((.9)*len(pos_tweets))] + neg_tweets[:int((.9)*len(neg_tweets))]\n",
    "train_data = pos_tweets[:int((.9)*len(pos_tweets))] + neg_tweets[:int((.9)*len(neg_tweets))] + negative_data[:int((.9)*len(negative_data))] + positive_data[:int((.9)*len(positive_data))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = pos_tweets[:int((.1)*len(pos_tweets))] + neg_tweets[:int((.1)*len(neg_tweets))]\n",
    "test_data =  pos_tweets[:int((.1)*len(pos_tweets))] + neg_tweets[:int((.1)*len(neg_tweets))] + negative_data[:int((.1)*len(negative_data))] + positive_data[:int((.1)*len(positive_data))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mNaiveBayesClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m classifier\u001b[38;5;241m.\u001b[39mshow_most_informative_features()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/classify/naivebayes.py:210\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.train\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    206\u001b[0m fnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Count up how many times each feature value occurred, given\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# the label and featurename.\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m featureset, label \u001b[38;5;129;01min\u001b[39;00m labeled_featuresets:\n\u001b[1;32m    211\u001b[0m     label_freqdist[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname, fval \u001b[38;5;129;01min\u001b[39;00m featureset\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# Increment freq(fval|label, fname)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/classify/util.py:91\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(classifier, gold)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(classifier, gold):\n\u001b[0;32m---> 91\u001b[0m     results \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mclassify_many([fs \u001b[38;5;28;01mfor\u001b[39;00m (fs, l) \u001b[38;5;129;01min\u001b[39;00m gold])\n\u001b[1;32m     92\u001b[0m     correct \u001b[38;5;241m=\u001b[39m [l \u001b[38;5;241m==\u001b[39m r \u001b[38;5;28;01mfor\u001b[39;00m ((fs, l), r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(gold, results)]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m correct:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "class_accuracy = accuracy(classifier, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connotation: negative with accuracy of 95%'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_text = 'Poor cavs....all that hard work.'\n",
    "pos_text = 'Now I can breathe haha '\n",
    "\n",
    "def classify_sentence(input):\n",
    "    m_accuracy = f'{int(class_accuracy * 100)}%'\n",
    "    sentiment_class = classifier.classify(token_sentence(input))\n",
    "    return f\"Connotation: {sentiment_class} with accuracy of {m_accuracy}\"\n",
    "\n",
    "classify_sentence(first_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523       simply copy and paste the following text into ...\n",
       "524       in order to help increase the booklets downloa...\n",
       "525       as of the booklet had been downloaded over tim...\n",
       "527          click on the download my bad green banner link\n",
       "528                                      booklet updated on\n",
       "                                ...                        \n",
       "440894    crash another movie from left field i have to ...\n",
       "440895    i why do not you debate first before starting ...\n",
       "440896    removal of i reverted the removal of the above...\n",
       "440897    i have unblocked you eddie as i discussed on u...\n",
       "440898    you have the ability to delete that revision a...\n",
       "Name: Content, Length: 361594, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv('HateSpeechDatasetBalanced.csv')\n",
    "\n",
    "# positive_data = data[data['Label'] == 0]\n",
    "# negative_data = data[data['Label'] == 1]\n",
    "\n",
    "# positive_data = positive_data['Content']\n",
    "# negative_data = negative_data['Content']\n",
    "\n",
    "# positive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 Not Found\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "url = \"https://twitter.com/elonmusk/status/1748595664131363231\"\n",
    "\n",
    "driver=webdriver.Firefox()\n",
    "driver.minimize_window()\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "resp = driver.page_source\n",
    "driver.close()\n",
    "\n",
    "tweet_soup = BeautifulSoup(resp, 'html.parser')\n",
    "\n",
    "\n",
    "try:\n",
    "    tweet = tweet_soup.find(\"div\",{\"data-testid\":\"tweetText\"})\n",
    "    tweet_text = tweet.find('span', class_='css-1qaijid r-bcqeeo r-qvutc0 r-poiln3').text\n",
    "    print(tweet_text)\n",
    "\n",
    "except:\n",
    "    tweet = None\n",
    "    print('404 Not Found')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
